{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conversion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2PJr-uVs_vz-"
      ],
      "authorship_tag": "ABX9TyO3DFJCQAzVpc8jKZAy++ll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/MLPMixer-jax2tf/blob/main/conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z4Qn49_3XMn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMJLyEVZrfTf"
      },
      "source": [
        "!pip install -q absl-py>=0.12.0 chex>=0.0.7 clu>=0.0.3 einops>=0.3.0\n",
        "!pip install -q flax==0.3.3 ml-collections==0.1.0 tf-nightly\n",
        "!pip install -q numpy>=1.19.5 pandas>=1.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaVk-KhKrsUG"
      },
      "source": [
        "# Clone repository and pull latest changes.\n",
        "![ -d vision_transformer ] || git clone --depth=1 https://github.com/sayakpaul/vision_transformer -b mixer-b32\n",
        "!cd vision_transformer && git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6evvE_N3ZLr"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxyovdLprv77"
      },
      "source": [
        "import sys\n",
        "\n",
        "if \"./vision_transformer\" not in sys.path:\n",
        "    sys.path.append(\"./vision_transformer\")\n",
        "\n",
        "from vit_jax import models\n",
        "from vit_jax import checkpoint\n",
        "from vit_jax.configs import common as common_config\n",
        "from vit_jax.configs import models as models_config\n",
        "\n",
        "from jax.experimental import jax2tf\n",
        "import tensorflow as tf\n",
        "import flax\n",
        "import jax\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F5FnKXytefW"
      },
      "source": [
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"FLAX version: {flax.__version__}\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWZyHFkj3aue"
      },
      "source": [
        "## Select model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9CawxFytiFc"
      },
      "source": [
        "#@title Choose a model type\n",
        "MIXER_MODELS = \"B_32\" #@param [\"L_16\", \"B_16\", \"B_32\"]\n",
        "DATASET = \"imagenet1k\" #@param [\"imagenet1k\", \"imagenet21k\"]\n",
        "SAM_PRETRAINED = True #@param {type:\"boolean\"}\n",
        "\n",
        "if SAM_PRETRAINED and (MIXER_MODELS == \"L_16\" or DATASET == \"imagenet-21k\"):\n",
        "    raise ValueError(f\"{MIXER_MODELS} and {DATASET} checkpoints are not available for SAM pre-training.\") \n",
        "elif not SAM_PRETRAINED and MIXER_MODELS == \"B_32\":\n",
        "    raise ValueError(f\"{MIXER_MODELS} is only available with SAM.\")\n",
        "else:\n",
        "    print(f\"Model type selected: Mixer-{MIXER_MODELS}\")\n",
        "    print(f\"Dataset selected: {DATASET}\")\n",
        "\n",
        "ROOT_GCS_PATH = \"gs://mixer_models/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Xg9AFkwMX5"
      },
      "source": [
        "classification_model = True\n",
        "num_classes_map = {\n",
        "    \"imagenet1k\": 1000,\n",
        "    \"imagenet21k\": 21843\n",
        "}\n",
        "\n",
        "if classification_model:\n",
        "    if not SAM_PRETRAINED:\n",
        "        num_classes = num_classes_map[DATASET]\n",
        "    else:\n",
        "        num_classes = 1000\n",
        "    print(f\"Will be converting a classification model with {num_classes} classes.\")\n",
        "else:\n",
        "    num_classes = None\n",
        "    print(\"Will be converting a feature extraction model.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJyjthX3fAi"
      },
      "source": [
        "## Instantiate model class and load checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMuUJNDkwNB9"
      },
      "source": [
        "# Instantiate model class and load the corresponding checkpoints.\n",
        "model_config = models_config.MODEL_CONFIGS[f\"Mixer-{MIXER_MODELS}\"]\n",
        "model = models.MlpMixer(num_classes=num_classes, **model_config)\n",
        "\n",
        "if SAM_PRETRAINED:\n",
        "    path = f\"{ROOT_GCS_PATH}sam/Mixer-{MIXER_MODELS}.npz\"\n",
        "else:\n",
        "    path = f\"{ROOT_GCS_PATH}{DATASET}/Mixer-{MIXER_MODELS}.npz\"\n",
        "\n",
        "params = checkpoint.load(path)\n",
        "\n",
        "if not num_classes:\n",
        "    _ = params.pop(\"head\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSc3Cyi83j6f"
      },
      "source": [
        "## Run conversion\n",
        "\n",
        "Code has been reused from the official examples [here](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/README.md)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1lyYMnuyZ0D"
      },
      "source": [
        "predict_fn = lambda params, inputs: model.apply(\n",
        "    dict(params=params), inputs, train=False\n",
        ")\n",
        "\n",
        "with_gradient = False if num_classes else True\n",
        "tf_fn = jax2tf.convert(\n",
        "    predict_fn,\n",
        "    with_gradient=with_gradient,\n",
        "    polymorphic_shapes=[None, \"b, 224, 224, 3\"],\n",
        "    enable_xla=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KENWNV1yelF"
      },
      "source": [
        "trainable = False if num_classes else True\n",
        "param_vars = tf.nest.map_structure(\n",
        "    lambda param: tf.Variable(param, trainable=trainable), params\n",
        ")\n",
        "tf_graph = tf.function(\n",
        "    lambda inputs: tf_fn(param_vars, inputs), autograph=False, jit_compile=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1QJQwDEyTs2V"
      },
      "source": [
        "#@title SavedModel wrapper class utility from [here](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/saved_model_lib.py#L128)\n",
        "class _ReusableSavedModelWrapper(tf.train.Checkpoint):\n",
        "  \"\"\"Wraps a function and its parameters for saving to a SavedModel.\n",
        "  Implements the interface described at\n",
        "  https://www.tensorflow.org/hub/reusable_saved_models.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, tf_graph, param_vars):\n",
        "    \"\"\"Args:\n",
        "      tf_graph: a tf.function taking one argument (the inputs), which can be\n",
        "         be tuples/lists/dictionaries of np.ndarray or tensors. The function\n",
        "         may have references to the tf.Variables in `param_vars`.\n",
        "      param_vars: the parameters, as tuples/lists/dictionaries of tf.Variable,\n",
        "         to be saved as the variables of the SavedModel.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Implement the interface from https://www.tensorflow.org/hub/reusable_saved_models\n",
        "    self.variables = tf.nest.flatten(param_vars)\n",
        "    self.trainable_variables = [v for v in self.variables if v.trainable]\n",
        "    # If you intend to prescribe regularization terms for users of the model,\n",
        "    # add them as @tf.functions with no inputs to this list. Else drop this.\n",
        "    self.regularization_losses = []\n",
        "    self.__call__ = tf_graph\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWvKaJI8ygPs"
      },
      "source": [
        "input_signatures = [tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32)]\n",
        "\n",
        "if SAM_PRETRAINED:\n",
        "    model_dir = MIXER_MODELS + \"_sam\"\n",
        "else:\n",
        "    model_dir = MIXER_MODELS + f\"_{DATASET}\"\n",
        "\n",
        "model_dir = model_dir if num_classes else f\"{model_dir}_fe\"\n",
        "\n",
        "signatures = {}\n",
        "saved_model_options = None\n",
        "\n",
        "print(f\"Saving model to {model_dir} directory.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cSsmfM5y33_"
      },
      "source": [
        "signatures[\n",
        "    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n",
        "] = tf_graph.get_concrete_function(input_signatures[0])\n",
        "\n",
        "wrapper = _ReusableSavedModelWrapper(tf_graph, param_vars)\n",
        "if with_gradient:\n",
        "    if not saved_model_options:\n",
        "        saved_model_options = tf.saved_model.SaveOptions(\n",
        "            experimental_custom_gradients=True\n",
        "        )\n",
        "    else:\n",
        "        saved_model_options.experimental_custom_gradients = True\n",
        "tf.saved_model.save(\n",
        "    wrapper, model_dir, signatures=signatures, options=saved_model_options\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJr-uVs_vz-"
      },
      "source": [
        "## Functional test (credits: [Willi Gierke](https://ch.linkedin.com/in/willi-gierke))\n",
        "\n",
        "***Currently only applicable for ImageNet-1k.*** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA2G4HzvC5_l"
      },
      "source": [
        "### Image preprocessing utilities "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyvjkBAE5iFL"
      },
      "source": [
        "def preprocess_image(image):\n",
        "    image = np.array(image)\n",
        "    image_resized = tf.image.resize(image, (224, 224))\n",
        "    image_resized = tf.cast(image_resized, tf.float32)\n",
        "    image_resized = (image_resized - 127.5) / 127.5\n",
        "    return tf.expand_dims(image_resized, 0).numpy()\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    image = preprocess_image(image)\n",
        "    return image\n",
        "\n",
        "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt -O ilsvrc2012_wordnet_lemmas.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd-YH-hqAIQ9"
      },
      "source": [
        "### Load image and ImageNet-1k class mappings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vDQd6MEAEp_"
      },
      "source": [
        "with open(\"ilsvrc2012_wordnet_lemmas.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "imagenet_int_to_str = [line.rstrip() for line in lines]\n",
        "\n",
        "img_url = \"https://p0.pikrepo.com/preview/853/907/close-up-photo-of-gray-elephant.jpg\"\n",
        "image = load_image_from_url(img_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A-LxOYBANnv"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99DTGYB25o5d"
      },
      "source": [
        "# Load the converted SavedModel and check whether it finds the elephant.\n",
        "restored_model = tf.saved_model.load(model_dir)\n",
        "predictions = restored_model.signatures[\"serving_default\"](tf.constant(image))\n",
        "logits = predictions[\"output_0\"][0]\n",
        "predicted_label = imagenet_int_to_str[int(np.argmax(logits))]\n",
        "expected_label = \"Indian_elephant, Elephas_maximus\"\n",
        "assert (\n",
        "    predicted_label == expected_label\n",
        "), f\"Expected {expected_label} but was {predicted_label}\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}